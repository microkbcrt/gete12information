import requests
import json
from datetime import datetime
from bs4 import BeautifulSoup
import os

# --- é…ç½®åŒº ---
HOST_MID = "1636034895"
OUTPUT_FILENAME = 'latest_bili_post.json'
SESSDATA = os.environ.get('BILI_SESSDATA')

def scrape_opus_page_content(opus_url, headers):
    """
    å¢å¼ºç‰ˆæŠ“å–å‡½æ•°ï¼šå¦‚æœæ‰¾ä¸åˆ°ç›®æ ‡å®¹å™¨ï¼Œåˆ™è¾“å‡ºæ•´ä¸ªHTMLé¡µé¢ç”¨äºè°ƒè¯•ã€‚
    """
    print(f"æ­£åœ¨æŠ“å–åŠ¨æ€è¯¦æƒ…é¡µ: {opus_url}")
    try:
        response = requests.get(opus_url, headers=headers, timeout=15)
        response.raise_for_status()
        
        # è·å–å“åº”çš„æ–‡æœ¬å†…å®¹
        html_content = response.text

        soup = BeautifulSoup(html_content, 'lxml')
        content_div = soup.find('div', class_='opus-module-content')

        if content_div:
            # æ‰¾åˆ°äº†ï¼Œæ­£å¸¸è¿”å›æ–‡æœ¬
            print("æˆåŠŸæ‰¾åˆ° 'opus-module-content' å®¹å™¨ã€‚")
            full_text = content_div.get_text(separator='\n', strip=True)
            return full_text
        else:
            # --- å…³é”®çš„è¯Šæ–­ä»£ç  ---
            # æ²¡æ‰¾åˆ°ï¼Œæ‰“å°æ•´ä¸ªHTMLé¡µé¢åˆ°æ—¥å¿—ä¸­
            print("\n" + "="*50)
            print("ã€è¯Šæ–­ä¿¡æ¯ã€‘æœªèƒ½åœ¨é¡µé¢ä¸­æ‰¾åˆ° 'opus-module-content' å®¹å™¨ã€‚")
            print("è¿™å¾ˆå¯èƒ½æ˜¯å› ä¸ºBç«™è¿”å›äº†ä¸€ä¸ªç™»å½•é¡µã€éªŒè¯é¡µæˆ–é”™è¯¯é¡µã€‚")
            print("ä»¥ä¸‹æ˜¯GitHub Actionså®é™…æŠ“å–åˆ°çš„å®Œæ•´HTMLé¡µé¢å†…å®¹ï¼š")
            print("="*50 + "\n")
            print(html_content) # æ‰“å°å®Œæ•´HTML
            print("\n" + "="*50)
            print("ã€è¯Šæ–­ä¿¡æ¯ç»“æŸã€‘è¯·æ£€æŸ¥ä¸Šé¢çš„HTMLå†…å®¹ä»¥ç¡®å®šé—®é¢˜åŸå› ã€‚")
            print("="*50 + "\n")
            
            # ä»ç„¶è¿”å›ä¸€ä¸ªæ˜ç¡®çš„é”™è¯¯ä¿¡æ¯å†™å…¥JSON
            return "[æŠ“å–é”™è¯¯] æœªèƒ½æ‰¾åˆ°ç›®æ ‡å†…å®¹å®¹å™¨ã€‚è¯·æŸ¥çœ‹Actionsæ—¥å¿—ä¸­çš„è¯Šæ–­ä¿¡æ¯ã€‚"

    except requests.exceptions.RequestException as e:
        print(f"æŠ“å–è¯¦æƒ…é¡µå¤±è´¥: {e}")
        return f"[ç½‘ç»œé”™è¯¯] æ— æ³•è®¿é—®åŠ¨æ€è¯¦æƒ…é¡µ: {opus_url}"
    except Exception as e:
        print(f"è§£æHTMLæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return "[è§£æé”™è¯¯] è§£æè¯¦æƒ…é¡µHTMLæ—¶å‡ºé”™ã€‚"

def fetch_and_process_dynamics():
    # ... (è¿™éƒ¨åˆ†ä¸»å‡½æ•°å’Œä¹‹å‰å®Œå…¨ä¸€æ ·ï¼Œæ— éœ€ä¿®æ”¹) ...
    api_url = f"https://api.bilibili.com/x/polymer/web-dynamic/v1/feed/space?host_mid={HOST_MID}"

    if not SESSDATA:
        print("é”™è¯¯ï¼šç¯å¢ƒå˜é‡ BILI_SESSDATA æœªè®¾ç½®ï¼")
        exit(1)

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',
        'Cookie': f'SESSDATA={SESSDATA}',
        'Referer': f'https://space.bilibili.com/{HOST_MID}/dynamic',
        # å°è¯•æ·»åŠ æ›´å¤šè¯·æ±‚å¤´ï¼Œè®©è¯·æ±‚çœ‹èµ·æ¥æ›´åƒçœŸå®æµè§ˆå™¨
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
        'Connection': 'keep-alive',
    }

    print(f"ç¬¬ä¸€æ­¥ï¼šæ­£åœ¨é€šè¿‡APIè·å–ç”¨æˆ·(UID: {HOST_MID})çš„åŠ¨æ€åˆ—è¡¨...")
    
    try:
        response = requests.get(api_url, headers=headers, timeout=10)
        response.raise_for_status()
        data = response.json()
        
        if data.get('code') != 0:
            print(f"APIè¿”å›é”™è¯¯ï¼ ä»£ç : {data.get('code')}, ä¿¡æ¯: {data.get('message', 'æ— ')}")
            exit(1)

        items_list = data.get('data', {}).get('items')
        if not items_list:
            print("APIè¿”å›çš„æ•°æ®ä¸­æœªæ‰¾åˆ°åŠ¨æ€åˆ—è¡¨ã€‚")
            return

        latest_post = max(items_list, key=lambda item: item.get('modules', {}).get('module_author', {}).get('pub_ts', 0))
        
        dynamic_id = latest_post.get('id_str')
        if not dynamic_id:
            print("é”™è¯¯ï¼šæ— æ³•ä»æœ€æ–°çš„åŠ¨æ€ä¸­æå–åˆ° 'id_str'ã€‚")
            exit(1)

        opus_url = f"https://www.bilibili.com/opus/{dynamic_id}"
        print(f"\nç¬¬äºŒæ­¥ï¼šå·²æ‰¾åˆ°æœ€æ–°åŠ¨æ€ID({dynamic_id})ï¼Œå‡†å¤‡æŠ“å–è¯¦æƒ…é¡µã€‚")
        
        final_content = scrape_opus_page_content(opus_url, headers)
        
        publish_timestamp = latest_post.get('modules', {}).get('module_author', {}).get('pub_ts', 0)
        publish_time_str = datetime.fromtimestamp(publish_timestamp).strftime('%Y-%m-%d %H:%M:%S')

        final_data = {
            "publish_time": publish_time_str,
            "dynamic_url": opus_url,
            "content": final_content
        }

        with open(OUTPUT_FILENAME, 'w', encoding='utf-8') as f:
            json.dump(final_data, f, ensure_ascii=False, indent=4)

        print(f"\nğŸ‰ ä»»åŠ¡å®Œæˆï¼è¯¦ç»†å†…å®¹å·²ä¿å­˜åˆ°æ–‡ä»¶: {OUTPUT_FILENAME}")
        
    except requests.exceptions.RequestException as e:
        print(f"APIè¯·æ±‚å¤±è´¥: {e}")
        exit(1)
    except Exception as e:
        print(f"å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        exit(1)

if __name__ == "__main__":
    fetch_and_process_dynamics()
